---
title: "VIP_Preliminary_Content_Analysis"
author: "Katie Murenbeeld"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}

library(RCurl)
library(XML)
library(stringr)
library(rvest)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(pdftools)
library(tm)
library(reshape2)
library(wordcloud)
library(forcats)
library(topicmodels)
library(tidyr)
library(formatR)

knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = TRUE)
```

# VIP Fall 2022/Spring 2023

## Content Analysis and Topic Modeling of Wildlife Conservation Articles

Intro text...


## Creating the Corpus (and some preprocessing)

A corpus is... In this case the corpus is the collection of roughly 50 articles that were reviewed in the VIP class in the fall of 2022. 

Through a bit of trial and error I found that using this bit of code created a corpus with the some extra lines of text which contained information about the article title, the newspaper, date, and page, all of which is also in the file name. Because of this I decided to removed the first two lines of text and the line of text with the copyright information from the files before processing into a corpus. 

```{r sample_corp, echo=TRUE, eval=FALSE}

# Load in the files as a list of file names.
# In this case all of the files are pdfs and they all are in the working directory.
files <- list.files(pattern = "pdf$")

corp <- Corpus(URISource(files), # First argument is what we want to use to create 
               # the corpus, the vector of pdf files. 
               # URI (uniform resource identifier) source means that the vector of 
               # file names identifies our resources
               readerControl = list(reader = readPDF)) # Second argument requires a 
               # list of control parameters, one is reader, to read in PDF files.

```

In this code block the files are loaded the same as before, but we will remove the first two lines of text and the copyright information before building the corpus. 

```{r make_corpus, echo=TRUE, EVAL=TRUE}

# Load in the files as a list of file names.
# In this case all of the files are pdfs and they all are in the working directory.
files <- list.files(pattern = "pdf$")

# Use lapply to to extract the text from all of the pdf files in the list
vip <- lapply(files, pdf_text) 

# Want to remove the first 2 lines of text which contains the title, newspaper, and page number
# Want to remove the line of text which has the copyright info
for (x in 1:length(vip)){
  #print(x)
  vip[[x]] <- str_replace(vip[[x]], regex("[a-zA-Z0-9_].*[\n]"), "")
  vip[[x]] <- str_replace(vip[[x]], regex("[a-zA-Z0-9_].*[\n]"), "")
  vip[[x]] <- str_replace(vip[[x]], regex("Copyright.*[\n]"), "")
}

# Next, build the corpus using the Corpus() function from the tm package
vip_corp <- Corpus(VectorSource(vip))

```

Now that we have a corpus of news articles we can create a Document Term Matrix (DTM). From the DTM we can create a tidy data frame.

## Creating and Document Term Matrix and a Tidy Dataframe

```{r dtm_tidy, echo=TRUE, eval=TRUE}

# From this corpus, create a document term matrix.
# The DTM will be useful for topic modeling but can be converted into a tidy data frame 
# for descriptive and exploratory analysis

vip.dtm <- DocumentTermMatrix(vip_corp, 
                              control = 
                                list(removePunctuation = TRUE, # removes all punctuation
                                     stopwords = TRUE, # removes common "stop words"
                                     tolower = TRUE, # makes all the characters lower case
                                     # here we did not "stem" the words 
                                     #(for example: "killed" and "killing" are not turned into the stem word "kill")
                                     stemming = FALSE, 
                                     removeNumbers = FALSE, # we kept numbers
                                     bounds = list(global = c(1, Inf)))) # count words that appear in at least 1 document


# Have a look at the dtm
inspect(vip.dtm)

# Now we can convert the dtm to a tidy dataframe
vip.tidy <- tidy(vip.dtm)
head(vip.tidy, 5)
```

One of the problems with building the corpus this way instead of from the files is that we lose the information in the document column. Here the document column has a number for each article, but we want the actual file name. 

## (More) Text preprocessing

```{r preprocess, echo=TRUE, eval=TRUE}

# Make the data in the document a Number
vip.tidy$document <- as.numeric(vip.tidy$document)

# Replace the document number with the document name from the list of files
for (x in 1:length(files)) {
  vip.tidy$document[vip.tidy$document == x] <- files[x]
}

# make the characters in the document column all lowercase
vip.tidy$document <- tolower(vip.tidy$document)
```


### Parsing out information from the document file name and adding an animal label

One thing that may be helpful for the content analysis is to take the information found in the document column and parse it out into other data columns. For example the file name contains the name of the article, the name of the newspaper, the date published, and the page number. Here we will take the name of the article, the newspaper and the date and put all of that information into three new columns. We will also create a column called "animal" which will make a rough approximation of whether the article was about bears or wolves based on the article title. 

Luckily for use, this information is split by some character patterns. The article title is the first string of characters followed by two underscores __, the newspaper is between two underscores and three underscores, and the date can be parsed out because of the pattern of letters and numbers.

We also look at the files name, and if "bear" is in the name the animal column will be set to "bear" and if "bear" is not in the document name the animal column will be set to "wolf". This is a pretty rough approximation, but we can come up with a better way to do this later.

```{r parse_filename, echo=TRUE, eval=TRUE}
vip.tidy <- vip.tidy %>%
  mutate(title = str_extract(document, regex("(.*?)(?=__)"))) %>%
  mutate(newspaper = str_extract(document, regex("(?<=__)[a-zA-Z]+-?_?.*(?=___)"))) %>%
  mutate(date = str_extract(document, "[a-zA-Z]+_[0-9]+_[0-9]+(?=__)")) %>%
  mutate(animal = ifelse(str_detect(vip.tidy$document, "bear") == TRUE, "bear", "wolf"))
head(vip.tidy, 5)
```

### Final tidy dataframe

Is there any other information we would want to include in the data frame? 

Also, can we come up with a more efficient way to build a corpus? I ended up downloading each article as a pdf from newsbank. I had to do this because the format of the html from newsbank was beyond my beginner skills at web scrapping and text mining. While downloading the pdfs did not take long for approximately 50 articles, I imagine it may be more challenging with 1000s of articles. 

## Content Analysis

What is content analysis...

### Let's explore the corpus

What are the 20 most common words in the corpus?

```{r corp_top_words, echo=FALSE, eval=TRUE}
vip.tidy %>%
  count(term, sort = TRUE) %>%
  slice_max(n, n=20) %>% 
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(n, term)) +
  geom_col() +
  labs(title = "20 Most Common Words in the VIP Corpus",
       y = NULL)
```
Should we make our own list of stop words? For example, how helpful are words like *"will"*, *"can"*, *"said"* and *"also"* to our content analysis and topic modeling?

### Sentiment analysis of the corpus

Sentiment analysis...valence...

Different sentiment lexicons. Here we will use the Bing (cite) lexicon which provides a rating of positive or negative sentiment to words.

```{r sentiment, echo=FALSE, eval=TRUE}

# First, join with the Bing sentiment lexicon.
vip.sent <- vip.tidy %>%
  inner_join(get_sentiments("bing"), by = c(term = "word")) %>%
  count(document, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

## Try to relabel the x axis. 
vip.sent$id <- row.names(vip.sent)
vip.sent$id <- as.numeric(vip.sent$id)
ggplot(vip.sent, aes(id, sentiment, fill = sentiment)) + 
  geom_col(show.legend = FALSE)
```

What are the most common positive and negative words? 

```{r sent_words, echo=FALSE, eval=TRUE}
vip.bing.counts <- vip.tidy %>%
  inner_join(get_sentiments("bing"), by = c(term = "word")) %>%
  count(term, sentiment, sort = TRUE) %>%
  ungroup()

vip.bing.counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(n, term, fill = sentiment)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment", 
       y = NULL)
```
From this list of words we may want to consider "stemming" as described above. The Bing lexicon is very useful in this case since we only labeled an article code as either positive or negative. In the future we may want to consider using a different lexicon. 

Another fun visualization of content analysis is a word cloud. 

```{r sent_wordcloud, echo=FALSE, eval=TRUE}
vip.tidy %>%
  inner_join(get_sentiments("bing"), by = c(term = "word")) %>%
  count(term, sentiment, sort = TRUE) %>%
  acast(term ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```






How well does the sentiment analysis in R match the sentiment analysis we coded in Fall 2022? I made a simplified version of the Article Coding spreadsheet from the VIP Fall 2022 class with three columns: Article Title, Species, and Code1_valence. I need to think about this more. May be challenging to match the data up to correct article (because of the text format).

### Analyzing word and document frequency: tf-idf

Info about word and document frequency (term frequency - inverse document frequency tf-idf).

```{r tf-idf1, echo=FALSE, eval=TRUE}

# Take a subset of the documents for this part
#vip.tidy$id <- row.names(vip.tidy)
#vip.tidy$id <- as.numeric(vip.tidy$id)
#vip.tidy <- subset(vip.tidy, id <= 10)

# We need to order the table by count first
vip.words <- vip.tidy %>%
  arrange(desc(count))

total.words <- vip.words %>%
  group_by(document) %>%
  summarise(total = sum(count))

vip.words <- left_join(vip.words, total.words)

vip.freq.rank <- vip.words %>%
  mutate(rank = row_number(),
         'term_frequency' = count/total) %>%
  ungroup()

vip.freq.rank %>%
  ggplot(aes(rank, term_frequency, color=document)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```
We could also group the corpus by the animal topic of the article.

```{r tf-idf2, echo=FALSE, eval=TRUE}
# We need to order the table by count first
vip.words <- vip.tidy %>%
  arrange(desc(count))

total.words <- vip.words %>%
  group_by(animal) %>%
  summarise(total = sum(count))

vip.words <- left_join(vip.words, total.words)

vip.freq.rank <- vip.words %>%
  mutate(rank = row_number(),
         'term_frequency' = count/total) %>%
  ungroup()

vip.freq.rank %>%
  ggplot(aes(rank, term_frequency, color=animal)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() +
  scale_y_log10()
```

```{r tf-idf_bind, echo=FALSE, eval=TRUE}
vip.tf.idf <- vip.words %>%
  bind_tf_idf(term, document, count)

vip.tf.idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))

vip.tf.idf %>%
  group_by(document) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  mutate(term = reorder(term, tf_idf)) %>%
  ggplot(aes(tf_idf, term, fill = document)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = "tf-idf", y = NULL) + 
  facet_wrap(~document, ncol = 5, scales = "free")
```

## Topic Modeling

Topic modeling...

### Latent Dirichlet Allocation (LDA)

Latent dirichlet allocation...

Let's start with 2 topics, see if the model can identify articles as relating to bears or wolves. 

```{r lda_2_beta, echo=FALSE, eval=TRUE}

vip.dtm2 <- vip.tidy %>%
  cast_dtm(document, term, count) # recast, not sure why, but otherwise I get a funny error

vip.lda2 <- LDA(vip.dtm2, k = 2, control = list(seed = 1234))

vip.topics2 <- tidy(vip.lda2, matrix = "beta")

vip.top.terms2 <- vip.topics2 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) # arrange sorts the data

vip.top.terms2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") + 
  scale_y_reordered()


```

```{r lda_2_gamma, echo=FALSE, eval=TRUE}

vip.docs2 <- tidy(vip.lda2, matrix = "gamma")

vip.docs2 %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma))

```

How well did the classifer work?

```{r lda_2_classify, echo=FALSE, eval=TRUE}

vip.docs2.classification <- vip.docs2 %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

vip.docs2.topics <- vip.tidy %>%
  group_by(document) %>%
  mutate(animal_orig = animal) %>%
  count(document, animal_orig) %>%
  group_by(document) %>%
  select(-n)%>%
  ungroup() 

vip.docs2.topics$topic <- ifelse(vip.docs2.topics$animal_orig == "wolf",1,2)
vip.docs2.classification$animal_mod <- ifelse(vip.docs2.classification$topic == 1, "wolf", "bear")
vip.docs2.classification$document <- tolower(vip.docs2.classification$document)
vip.docs2.misclass <- vip.docs2.classification %>%
  inner_join(vip.docs2.topics, by = "document") %>%
  filter(animal_orig != animal_mod)

vip.docs2.misclass %>%
  count(animal_orig)
```

The model didn't work that well. 24 of the 47 articles were misclassified. The vast majority were articles that were supposed to be about wolves. This may very well have to do with how I classified the articles (was bear in the article name, if not label the article as wolf).

Let's do 16 topics (the number of codes in the class code book).

```{r lda_16_beta, echo=FALSE, eval=TRUE}
vip.lda16 <- LDA(vip.dtm2, k = 16, control = list(seed = 1234))

vip.topics16 <- tidy(vip.lda16, matrix = "beta")

vip.top.terms16 <- vip.topics16 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% # Some have more than 10 because the beta values are "tied"
  ungroup() %>%
  arrange(topic, -beta) # arrange sorts the data

vip.top.terms16 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") + 
  scale_y_reordered()
```

```{r lda_16_gamma, echo=FALSE, eval=TRUE}
vip.docs16 <- tidy(vip.lda16, matrix = "gamma")
vip.docs16

vip.docs16 %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma))
```

```{r lda_16_classify, echo=FALSE, eval=TRUE}
vip.docs16.classification <- vip.docs16 %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

vip.docs16.classification %>%
  count(topic) %>%
  ggplot(aes(topic, n, fill=n)) + 
  geom_bar(stat="identity")

```






