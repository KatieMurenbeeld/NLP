---
title: "USR_2023_Figures"
author: "Katie Murenbeeld"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) #this is actually a whole set of packages designed to different data manipulation tasks. We rely on the dplyr package alot for organizing data and the stringr package for dealing with text
library(httr) # for accessing the html from the link
library(tidytext) # for creating a tidy dataframe from the website text data
library(wordcloud) # package for creating word clouds
#library(reshape2) # package to restructure and aggregate data
library(tm) # package for topic modeling
library(topicmodels)# package for topic modeling
library(SnowballC)
library(googledrive)
options(
  gargle_oauth_cache = ".secrets",
  gargle_oauth_email = TRUE
)
library(usmap)
```

```{r read}
# read in the csv with the urls for the articles
folder_url <- "https://drive.google.com/drive/u/0/folders/1ob5sagTtT3svhc7ZKeemd9TiAq1_MsCL"
folder <- drive_get(as_id(folder_url))

gdrive_files <- drive_ls(folder)
id <- gdrive_files[gdrive_files$name == "Article Coding", ]$id
drive_download(id, path = "data/article_coding.csv", overwrite = TRUE)

article_codes <- read.csv(file = 'data/article_coding.csv')
```

```{r cleancodes}
article_codes <- article_codes %>% 
  filter(!Link=='') %>% 
  filter(!Species == "Wolves") %>%
  filter(!Article.Type == "NA") %>%
  #filter(!Article.Title == "Designers say hello to comfort and individuality - into the 90s") %>%
  mutate(id = row_number())

# create a vector or list of the urls from the csv
urls <- article_codes$Link
```

```{r downloadarticles}
df_article <- data.frame()

### IMPORTANT!!! Must be connected to VPN or on campus for this to work! ###

for (url in urls){ # this was working, but now I am getting an error: Error in content(page, as = "text"): unused argument ("text")
  if(url == "") next
  link <- url
  #if(link == "") next
  page <- GET(link)
  page_text <- httr::content(page, as = 'text')
  text_body <- str_extract_all(page_text, regex("(?<=<p>).+(?=</p>)"), simplify = TRUE) 
  text <- text_body[[2]]
  df_article <- rbind(df_article, text)
}

# Rename and reorder the data frame columns
colnames(df_article) <- c("article.text")
df_article <- df_article %>% mutate(id = row_number()) %>% select(id, article.text)

```

```{r joindata}
# from the article_codes data frame select one or more variables/columns you would like to join to df_article
# you can update the columns in select() to include whatever information you'd like
# for example, article type, newspaper, etc. But you need the "id" column so that you can "join" the 2 data frames
df_to_join <- article_codes %>% select(id, Species, Code1) 
df_article <- df_article %>% full_join(df_to_join, by = "id")
```

```{r txtcln}
# Want to take out the Wolves articles
#df_article_nowolf <- df_article[-which(df_article$Species == "Wolves"), ]


# Make a tidy data frame by "unnesting" the tokens
# This automatically removes punctuation and will lowercase all words
tidy_df <- df_article %>% 
  select(id, article.text, Species, Code1) %>%
  unnest_tokens(word, article.text)

# Remove "stop words" using the SMART lexicon 
data("stop_words")
tidy_df <- tidy_df %>%
  anti_join(stop_words)

# Create a small data frame of your own stop words for this project 
# ("br" and "strong" are part of the html formatting which should come out before making tidy df!)
# This needs to be a data frame so that you can use "anti-join()"
# You can add your own words to this list
my_stop_words <- data.frame(c("br", "strong")) 
colnames(my_stop_words) <-("word")

tidy_df <- tidy_df %>%
  anti_join(my_stop_words) %>% 
  mutate(stem = wordStem(word))

```

## Sentiment Analyses

```{r sentiment}
sent <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(Species) %>%
  count(stem, sentiment, sort = TRUE) %>%
  ungroup()

species_colors <- c("Beavers" = "#C83438", "Boars" = "#8FD7E0", "Grizzly Bear" = "#09929F")

tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(Species) %>%
  count(id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(id, sentiment, fill = Species)) + 
  geom_col(show.legend = TRUE) + 
  scale_x_continuous(name="Document ID", limits=c(1, 171)) +
  scale_y_continuous(name="Sentiment Score", limits=c(-300, 100)) +
  labs(title = paste("Document Sentiment: Articles About Beavers, Boars, \n and Grizzly Bears")) + 
  ylab("Sentiment Score") + xlab("Document ID") + 
  scale_fill_manual(values=species_colors) + 
  theme_bw() + 
  theme(plot.title = element_text(size=16), axis.text.x = element_text(size=14), 
        axis.text.y = element_text(size=14), axis.title.x = element_text(size=14),
        axis.title.y = element_text(size=14), legend.title = element_text(size=14),
        legend.text = element_text(size=14))


ggsave("sentiment_documents_03.png", width=7, height=5, dpi=300)

```
## Word cloud

```{r word_clouds}
png("beavers_wordcloud_03.png", width = 400, height = 400)
tidy_df %>%
  filter(id == 78) %>%
  inner_join(get_sentiments("bing")) %>%
  count(stem, sentiment, sort = TRUE) %>%
  pivot_wider(id_cols = stem, names_from = sentiment, values_from = n, values_fill = 0) %>%
  column_to_rownames(var="stem") %>% 
  as.matrix() %>% 
  comparison.cloud(colors = c("blue", "green"),
                   max.words = 50, match.colors = TRUE)
dev.off()

png("boars_wordcloud_03.png", width = 200, height = 200)
tidy_df %>%
  filter(id == 162) %>%
  inner_join(get_sentiments("bing")) %>%
  count(stem, sentiment, sort = TRUE) %>%
  pivot_wider(id_cols = stem, names_from = sentiment, values_from = n, values_fill = 0) %>%
  column_to_rownames(var="stem") %>% 
  as.matrix() %>% 
  comparison.cloud(colors = c("blue", "green"),
                   max.words = 30, match.colors = TRUE)
dev.off()
```



## Sentiment by Species (Common Positive and Negative Words)

```{r sent_com_words}
sent_test <- tidy_df %>%
  #filter(Species == "Grizzly Bear") %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(Species) %>%
  count(stem, sentiment, sort = TRUE) %>%
  ungroup()

# Use the sentiment word counts data frame to make the plot
# I think we want to use facet wrap here
sent_test %>%
  group_by(sentiment, Species) %>%
  slice_max(n, n = 5) %>%
  ungroup() %>%
  mutate(word = reorder(stem, n)) %>%
  ggplot(aes(n, word, fill = Species)) + 
  scale_fill_manual(values=species_colors) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~Species + sentiment , scales = "free_y", ncol = 2) +
  #facet_wrap(~factor(sentiment + Species, levels = c("positive", "negative"))) + # didn't work
  #facet_grid(Species ~ sentiment, scales = "free_y") + # doesn't actually free y axis 
  labs(x = "Contribution to sentiment", 
       y = NULL,
       title = "Most Common Negative and Positive Word Stems by Species") + 
  theme_bw() +
  theme(plot.title = element_text(size=16), axis.text.x = element_text(size=14), 
        axis.text.y = element_text(size=14), axis.title.x = element_text(size=14), 
        strip.text.x = element_text(size=14))
ggsave("sentiment_contribution_vert_03.png", width=6, height=10, dpi=300)

sent_for_plot <- sent_test %>%
  group_by(sentiment, Species) %>%
  slice_max(n, n = 5) %>%
  ungroup() %>%
  mutate(word = reorder(stem, n))

sent_for_plot$Species <- factor(sent_for_plot$Species, levels = c("Beavers","Boars","Grizzly Bear"))
sent_for_plot$sentiment <- factor(sent_for_plot$sentiment, levels = c("positive", "negative"))

options(repr.plot.width = 5, repr.plot.height = 10)
sent_for_plot %>%
  ggplot(aes(n, word, fill = Species)) + 
  scale_fill_manual(values=species_colors) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(.~sentiment + Species , scales = "free_y") +
  #facet_wrap(~factor(sentiment + Species, levels = c("positive", "negative"))) + # didn't work
  #facet_grid(Species ~ sentiment, scales = "free_y") + # doesn't actually free y axis 
  labs(x = "Contribution to sentiment", 
       y = NULL,
       title = "Most Common Negative and Positive Word Stems by Species") + 
  theme_bw() + 
  theme(plot.title = element_text(size=16), axis.text.x = element_text(size=14), axis.text.y = element_text(size=14), axis.title.x = element_text(size=14), strip.text.x = element_text(size=14)) 

ggsave("sentiment_contribution_horiz_03.png", width=10, height=6, dpi=300)
```


# Map 
Need a base map of states
Need three points within the states, one for each species
For data frame I need a count of the articles by species by state
  Columns: State Species Count
  
```{r map_data}

# Set up the data frame

df_state_to_join <- article_codes %>% select(id, Species, Code1, Publication.State) 
df_states <- df_article %>% full_join(df_state_to_join, by = "id")

df_states_counts <- df_states %>%
  filter(!Publication.State == "") %>%
  group_by(Species.x, Publication.State) %>%
  count(Publication.State, Species.x)

df_state_locs <- read_csv("states_centers.csv")

df_states_map <- df_states_counts %>%
  inner_join(df_state_locs, by = c('Publication.State'='state'))

```

```{r map}
state_info <- map_data("state")
us_map <- ggplot(data = state_info, mapping = aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = "black", fill = "white") + 
  coord_quickmap() + 
  theme_void()

map_with_data <- us_map + 
  geom_point(data = df_states_map, aes(x = longitude, y = latitude, color=Species.x, size=n, group = n), position=position_jitter(h=0.75,w=0.75))

map_with_data
```

```{r map_testing}

transformed_data <- usmap_transform(df_states_map, input_names = c("longitude", "latitude"),
  output_names = c("x", "y")) %>%
  rename(Species = Species.x)

#test_base <- plot_usmap()

test_with_data <- plot_usmap('states') + 
  geom_point(data = transformed_data, aes(x = x, y = y, group = Species, color=Species, size=n, shape=Species), position=position_dodge(width = 300000)) + 
  scale_size_area(breaks = c(1, 10, 20, 30),
                  limits = c(1, 32)) + 
 # scale_shape_manual(values=c(20, 16, 17)) +
  scale_color_manual(values=species_colors) + 
  labs(title = "Location of Articles Published in the USA", 
       #color = "Species:",
       size = "Number of Articles:") +
  theme(plot.title = element_text(size = 16),
        legend.position = "bottom", 
        legend.background = element_rect(fill="white",
                                         size=0.3, linetype="solid", 
                                         colour ="black"),
        legend.title = element_text(size = 14), 
        legend.text = element_text(size = 14))


test_with_data
ggsave("map_article_locations_03.png", width=10, height=5, dpi=300)
```


## Try to get Alaska and Hawaii on there nicely

Matt suggested looking at st_centroid()

I ended up using usmap (plot_usmap()) and transforming the data using usmap_transform()

I still need to fix Montana. There is a boar article there that keeps getting overshadowed by all the Bear articles. 







